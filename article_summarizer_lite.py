import requests
from bs4 import BeautifulSoup
import anthropic
import csv
from datetime import datetime
import time
import os
from typing import List, Dict, Optional

class ArticleSummarizer:
    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
    
    def fetch_article(self, url: str, timeout: int = 10) -> Optional[Dict[str, str]]:
        try:
            print(f"  ğŸ“¡ URL ìš”ì²­ ì¤‘...")
            response = requests.get(url, headers=self.headers, timeout=timeout)
            response.raise_for_status()
            print(f"  âœ“ ì‘ë‹µ ë°›ìŒ (ìƒíƒœ: {response.status_code})")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ - ë” ë„“ì€ ë²”ìœ„ë¡œ ê²€ìƒ‰
            title = None
            title_selectors = [
                'h1',  # ê°€ì¥ í”í•œ ì œëª© íƒœê·¸
                'h2',  # ë¶€ì œëª©ìœ¼ë¡œë„ ì‚¬ìš©
                '.article-title',
                '.news-title',
                '.title',
                'meta[property="og:title"]',
                'meta[name="title"]'
            ]
            
            for selector in title_selectors:
                element = soup.select_one(selector)
                if element:
                    title = element.get('content') if element.name == 'meta' else element.get_text(strip=True)
                    if title and len(title) > 5:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                        print(f"  âœ“ ì œëª© ì°¾ìŒ (ì„ íƒì: {selector}): {title[:50]}...")
                        break
            
            if not title:
                print(f"  âœ— ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                print(f"  ğŸ” ì‹œë„í•œ ì„ íƒì: {title_selectors}")
                return None
            
            # ë³¸ë¬¸ ì¶”ì¶œ - ë” ë„“ì€ ë²”ìœ„ë¡œ ê²€ìƒ‰
            content = None
            content_selectors = [
                'article',
                '.article-body',
                '.news-content',
                '.article-content',
                '#article-view-content-div',
                '.view-content',
                '.article_view'
            ]
            
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                    for tag in element.find_all(['script', 'style', 'nav', 'aside', 'header', 'footer']):
                        tag.decompose()
                    content = element.get_text(strip=True, separator='\n')
                    if content and len(content) > 100:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                        print(f"  âœ“ ë³¸ë¬¸ ì°¾ìŒ (ì„ íƒì: {selector}): {len(content)}ì")
                        break
            
            # ë³¸ë¬¸ ëª» ì°¾ìœ¼ë©´ p íƒœê·¸ ëª¨ë‘ ìˆ˜ì§‘
            if not content or len(content) < 100:
                print(f"  âš ï¸ ì„ íƒìë¡œ ë³¸ë¬¸ ëª» ì°¾ìŒ, p íƒœê·¸ ìˆ˜ì§‘ ì‹œë„...")
                paragraphs = soup.find_all('p')
                content = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                if content:
                    print(f"  âœ“ p íƒœê·¸ë¡œ ë³¸ë¬¸ ìˆ˜ì§‘: {len(content)}ì")
            
            if not content or len(content) < 50:
                print(f"  âœ— ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({len(content) if content else 0}ì)")
                return None
            
            print(f"  âœ… ê¸°ì‚¬ ìˆ˜ì§‘ ì„±ê³µ!")
            return {'title': title, 'content': content}
            
        except requests.exceptions.RequestException as e:
            print(f"  âŒ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {e}")
            return None
        except Exception as e:
            print(f"  âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def summarize_article(self, title: str, content: str) -> str:
        try:
            max_chars = 10000
            if len(content) > max_chars:
                content = content[:max_chars] + "..."
            
            prompt = f"""ë‹¤ìŒ ê¸°ì‚¬ë¥¼ 2ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. 

í˜•ì‹: ê° ì¤„ì€ "ã†"ë¡œ ì‹œì‘í•˜ëŠ” ë¶ˆë¦¿ í¬ì¸íŠ¸ í˜•íƒœ
ì˜ˆì‹œ:
ã†ì¤‘ëŒ€ì¬í•´ë²• ì‹œí–‰ê³¼ ì •ë¶€ íŠ¹ë³„ê°ë… ì˜í–¥ìœ¼ë¡œ í˜‘ë ¥ì—…ì²´ ì„ ì • ê¸°ì¤€ì—ì„œ 'ì•ˆì „ ì—­ëŸ‰' ë¹„ì¤‘ ê°•í™” 
ã†í¬ìŠ¤ì½”ì´ì•¤ì”¨ëŠ” ìƒìƒê¸°ê¸ˆÂ·ì•ˆì „ê´€ë¦¬ë¹„ ì§€ì›, í˜„ëŒ€ê±´ì„¤ì€ í˜‘ë ¥ì‚¬ ì•ˆì „í‰ê°€ ë°°ì  í™•ëŒ€ ë“± ì•ˆì „ ê²½ìŸ

ì œëª©: {title}

ë³¸ë¬¸:
{content}

ìš”ì•½:"""
            
            print(f"  ğŸ¤– Claude API í˜¸ì¶œ ì¤‘...")
            message = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=500,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            summary = message.content[0].text.strip()
            print(f"  âœ“ ìš”ì•½ ì™„ë£Œ: {summary[:50]}...")
            return summary
            
        except Exception as e:
            print(f"  âŒ ìš”ì•½ ì—ëŸ¬: {e}")
            return "ìš”ì•½ ì‹¤íŒ¨"
    
    def process_urls(self, urls: List[str], output_file: str = "articles_summary.csv", 
                     delay: float = 1.0, save_interval: int = 20):
        results = []
        total = len(urls)
        
        print(f"ğŸ“° ì´ {total}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘...")
        print(f"=" * 80)
        
        for idx, url in enumerate(urls, 1):
            print(f"\n[{idx}/{total}] ì²˜ë¦¬ ì¤‘: {url}")
            
            article = self.fetch_article(url)
            
            if article:
                summary = self.summarize_article(article['title'], article['content'])
                
                results.append({
                    'URL': url,
                    'ì œëª©': article['title'],
                    'ìš”ì•½': summary,
                    'ì²˜ë¦¬ì‹œê°„': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'ìƒíƒœ': 'ì„±ê³µ'
                })
            else:
                results.append({
                    'URL': url,
                    'ì œëª©': '',
                    'ìš”ì•½': '',
                    'ì²˜ë¦¬ì‹œê°„': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'ìƒíƒœ': 'ì‹¤íŒ¨'
                })
            
            # ì¤‘ê°„ ì €ì¥
            if idx % save_interval == 0 or idx == total:
                self._save_csv(results, output_file)
                success = sum(1 for r in results if r['ìƒíƒœ'] == 'ì„±ê³µ')
                fail = sum(1 for r in results if r['ìƒíƒœ'] == 'ì‹¤íŒ¨')
                print(f"\nğŸ’¾ ì§„í–‰ìƒí™© ì €ì¥: {idx}/{total} (ì„±ê³µ: {success}, ì‹¤íŒ¨: {fail})")
            
            if idx < total:
                time.sleep(delay)
        
        # ìµœì¢… í†µê³„
        success = sum(1 for r in results if r['ìƒíƒœ'] == 'ì„±ê³µ')
        fail = sum(1 for r in results if r['ìƒíƒœ'] == 'ì‹¤íŒ¨')
        
        print(f"\n{'=' * 80}")
        print(f"ğŸ‰ ì™„ë£Œ! ì´ {len(results)}ê°œ ì²˜ë¦¬ë¨")
        print(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼: {output_file}")
        print(f"\n=== ì²˜ë¦¬ ê²°ê³¼ í†µê³„ ===")
        print(f"âœ… ì„±ê³µ: {success}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {fail}ê°œ")
        print(f"{'=' * 80}")
    
    def _save_csv(self, results: List[Dict], output_file: str):
        """CSV ì €ì¥ (pandas ì—†ì´)"""
        with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
            if results:
                writer = csv.DictWriter(f, fieldnames=results[0].keys())
                writer.writeheader()
                writer.writerows(results)
